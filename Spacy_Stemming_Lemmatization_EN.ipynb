{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "    Ao pesquisar uma palavra-chave, ajuda se a pesquisa retornar variações da palavra, por exemplo, pesquisar \"barco\" também pode retornar \"barcos\" e \"barco\". Aqui, \"barco\" seria a ** haste ** para [barco, barco, barco, barcos].\n",
    "\n",
    "    Stemming é um método um tanto rudimentar para catalogar palavras relacionadas; essencialmente corta as letras do final até que o radical seja alcançado. Isso funciona muito bem na maioria dos casos, mas infelizmente o inglês tem muitas exceções onde um processo mais sofisticado é necessário. Na verdade, spaCy não inclui um lematizador, optando por confiar inteiramente na lematização. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porter Stemmer\n",
    "\n",
    "    Uma das ferramentas de lematização mais comuns - e eficazes - é o Algoritmo de Porter\n",
    "    O algoritmo emprega cinco fases de redução de palavras, cada uma com seu próprio conjunto de regras de mapeamento. Na primeira fase, regras simples de mapeamento de sufixo são definidas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['run','runner','running','ran','runs','easily','fairly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+' --> '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green> Observe como o lematizador reconhece \"runner\" como um substantivo, não uma forma verbal ou particípio. Além disso, os advérbios \"facilmente\" e \"razoavelmente\" derivam da raiz incomum \"easili\" e \"fairli\" </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Snowball Stemmer\n",
    "    \"Porter2 Stemmer\" ele oferece uma ligeira melhoria em relação à original Porter, tanto em lógica quanto em velocidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fair\n"
     ]
    }
   ],
   "source": [
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generous --> generous\n",
      "generation --> generat\n",
      "generously --> generous\n",
      "generate --> generat\n"
     ]
    }
   ],
   "source": [
    "words = ['generous','generation','generously','generate']\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green> Neste caso, o lematizador teve o mesmo desempenho que o Porter Stemmer, com a exceção de que tratou o radical de \"fairly\" de forma mais adequada com \"fair\" </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer:\n",
      "felicidade --> felicidad\n"
     ]
    }
   ],
   "source": [
    "print('Porter Stemmer:')\n",
    "\n",
    "words = ['joyful']\n",
    "for word in words:\n",
    "    print(word+' --> '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter2 Stemmer:\n",
      "joyful --> joy\n"
     ]
    }
   ],
   "source": [
    "print('Porter2 Stemmer:')\n",
    "\n",
    "words = ['joyful']\n",
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('/home/alana/Documentos/NLP/UDEMY/en_core_web_sm-2.2.0/en_core_web_sm/en_core_web_sm-2.2.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t 561228191312463089 \t -PRON-\n",
      "am \t AUX \t 10382539506755952630 \t be\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "runner \t NOUN \t 12640964157389618806 \t runner\n",
      "running \t VERB \t 12767647472892411841 \t run\n",
      "in \t ADP \t 3002984154512732771 \t in\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "race \t NOUN \t 8048469955494714898 \t race\n",
      "because \t SCONJ \t 16950148841647037698 \t because\n",
      "I \t PRON \t 561228191312463089 \t -PRON-\n",
      "love \t VERB \t 3702023516439754181 \t love\n",
      "to \t PART \t 3791531372978436496 \t to\n",
      "run \t VERB \t 12767647472892411841 \t run\n",
      "since \t SCONJ \t 10066841407251338481 \t since\n",
      "I \t PRON \t 561228191312463089 \t -PRON-\n",
      "ran \t VERB \t 12767647472892411841 \t run\n",
      "today \t NOUN \t 11042482332948150395 \t today\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(u\"I am a runner running in a race because I love to run since I ran today\")\n",
    "\n",
    "for token in doc1:\n",
    "    print(token.text, '\\t', token.pos_, '\\t', token.lemma, '\\t', token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green> Na frase acima, `running`,` run` e `run` apontam para o mesmo lema` run` (... 11841) para evitar duplicação. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Função para exibir lemas\n",
    "    função que exiba as informações que desejamos de forma mais organizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}} {token.lemma:<{22}} {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   561228191312463089     -PRON-\n",
      "saw          VERB   11925638236994514241   see\n",
      "eighteen     NUM    9609336664675087640    eighteen\n",
      "mice         NOUN   1384165645700560590    mouse\n",
      "today        NOUN   11042482332948150395   today\n",
      "!            PUNCT  17494803046312582752   !\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(u\"I saw eighteen mice today!\")\n",
    "\n",
    "show_lemmas(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I            PRON   561228191312463089     -PRON-\n",
      "am           AUX    10382539506755952630   be\n",
      "meeting      VERB   6880656908171229526    meet\n",
      "him          PRON   561228191312463089     -PRON-\n",
      "tomorrow     NOUN   3573583789758258062    tomorrow\n",
      "at           ADP    11667289587015813222   at\n",
      "the          DET    7425985699627899538    the\n",
      "meeting      NOUN   14798207169164081740   meeting\n",
      ".            PUNCT  12646065887601541794   .\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(u\"I am meeting him tomorrow at the meeting.\")\n",
    "\n",
    "show_lemmas(doc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embora a lematização olhe para o texto circundante para determinar a classe gramatical de uma determinada palavra, ela não categoriza frases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
