{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('/home/alana/Documentos/NLP/UDEMY/en_core_web_sm-2.2.0/en_core_web_sm/en_core_web_sm-2.2.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando padrões\n",
    "Na literatura, a frase 'solar power' pode aparecer como uma ou duas palavras, com ou sem hífen. Nesta seção, desenvolveremos um matcher chamado 'SolarPower' que encontra todos os três:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1 = [{'LOWER': 'solarpower'}]\n",
    "pattern2 = [{'LOWER': 'solar'}, {'LOWER': 'power'}]\n",
    "pattern3 = [{'LOWER': 'solar'}, {'IS_PUNCT': True}, {'LOWER': 'power'}]\n",
    "\n",
    "matcher.add('SolarPower', None, pattern1, pattern2, pattern3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aplicando o matcher a um objeto Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    A indústria de energia solar continua a crescer conforme a demanda por energia solar aumenta. Carros movidos a energia solar estão ganhando popularidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u'The Solar Power industry continues to grow as demand for solarpower increases. Solar-power cars are gaining popularity.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)]\n"
     ]
    }
   ],
   "source": [
    "found_matches = matcher(doc)\n",
    "\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    matcher retorna uma lista de tuplas. Cada tupla contém um ID para a correspondência, com tokens de início e fim que mapeiam para o span doc [Inicio: Fim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8656102463236116519 SolarPower 1 3 Solar Power\n",
      "8656102463236116519 SolarPower 10 11 solarpower\n",
      "8656102463236116519 SolarPower 13 16 Solar-power\n"
     ]
    }
   ],
   "source": [
    "for match_id, start, end in found_matches:\n",
    "    string_id = nlp.vocab.strings[match_id]\n",
    "    span = doc[start:end]\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurando opções de padrão e quantificadores\n",
    "Você pode tornar as regras de token opcionais passando um argumento `'OP': '*'`. Isso nos permite simplificar nossa lista de padrões:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefina os padrões:\n",
    "pattern1 = [{'LOWER': 'solarpower'}]\n",
    "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]\n",
    "\n",
    "\n",
    "# Remova os padrões antigos para evitar duplicação:\n",
    "matcher.remove('SolarPower')\n",
    "\n",
    "# Adicione o novo conjunto de padrões ao matcher 'SolarPower':\n",
    "matcher.add('SolarPower', None, pattern1, pattern2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8656102463236116519, 1, 3), (8656102463236116519, 10, 11), (8656102463236116519, 13, 16)]\n"
     ]
    }
   ],
   "source": [
    "found_matches = matcher(doc)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso encontrou os dois padrões de duas palavras, com e sem o hífen!\n",
    "\n",
    "Os seguintes quantificadores podem ser passados ​​para a chave `'OP'`:\n",
    "<table><tr><th>OP</th><th>Descrição</th> </tr>\n",
    "\n",
    "<tr> <td> <span> \\! </span> </td> <td> Negue o padrão, exigindo que corresponda exatamente 0 vezes </td> </tr>\n",
    "<tr> <td> <span>? </span> </td> <td> Torne o padrão opcional, permitindo que corresponda 0 ou 1 vez </td> </tr>\n",
    "<tr> <td> <span> \\ + </span> </td> <td> Exigir que o padrão corresponda 1 ou mais vezes </td> </tr>\n",
    "<tr> <td> <span> \\ * </span> </td> <td> Permitir que o padrão corresponda zero ou mais vezes </td> </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cuidado com os lemas!\n",
    "Se quiséssemos combinar \"solar power\" e \"solar powered\", pode ser tentador procurar o * lema * de \"powered\" e esperar que seja \"power\". Isso não é sempre o caso! O lema do * adjetivo * 'powered' ainda é 'powered':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1 = [{'LOWER': 'solarpower'}]\n",
    "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LEMMA': 'power'}]\n",
    "\n",
    "# Remova os padrões antigos para evitar duplicação:\n",
    "matcher.remove('SolarPower')\n",
    "\n",
    "# Adicione o novo conjunto de padrões ao matcher 'SolarPower':\n",
    "matcher.add('SolarPower', None, pattern1, pattern2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Energia solar movimenta carros movidos a energia solar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = nlp(u'Solar-powered energy runs solar-powered cars.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8656102463236116519, 0, 3), (8656102463236116519, 5, 8)]\n"
     ]
    }
   ],
   "source": [
    "found_matches = matcher(doc2)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern1 = [{'LOWER': 'solarpower'}]\n",
    "pattern2 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'power'}]\n",
    "pattern3 = [{'LOWER': 'solarpowered'}]\n",
    "pattern4 = [{'LOWER': 'solar'}, {'IS_PUNCT': True, 'OP':'*'}, {'LOWER': 'powered'}]\n",
    "\n",
    "matcher.remove('SolarPower')\n",
    "\n",
    "matcher.add('SolarPower', None, pattern1, pattern2, pattern3, pattern4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8656102463236116519, 0, 3), (8656102463236116519, 5, 8)]\n"
     ]
    }
   ],
   "source": [
    "found_matches = matcher(doc2)\n",
    "print(found_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outros atributos de token\n",
    "Além dos lemas, há uma variedade de atributos de token que podemos usar para determinar as regras de correspondência:\n",
    "<table><tr><th>Atributo</th><th>Descrição</th> </tr>\n",
    "\n",
    "<tr> <td> <span> `ORTH` </span> </td> <td> O texto literal exato de um token </td> </tr>\n",
    "<tr> <td> <span> `LOWER` </span> </td> <td> A forma em minúsculas do texto do token </td> </tr>\n",
    "<tr> <td> <span> `LENGTH` </span> </td> <td> O comprimento do texto do token </td> </tr>\n",
    "<tr> <td> <span> `IS_ALPHA`,` IS_ASCII`, `IS_DIGIT` </span> </td> <td> O texto do token consiste em caracteres alfanuméricos, caracteres ASCII, dígitos </td> </tr>\n",
    "<tr> <td> <span> `IS_LOWER`,` IS_UPPER`, `IS_TITLE` </span> </td> <td> O texto do token está em minúsculas, maiúsculas, titlecase </td> </tr>\n",
    "<tr> <td> <span> `IS_PUNCT`,` IS_SPACE`, `IS_STOP` </span> </td> <td> Token é pontuação, espaço em branco, palavra de interrupção </td> </tr>\n",
    "<tr> <td> <span> `LIKE_NUM`,` LIKE_URL`, `LIKE_EMAIL` </span> </td> <td> O texto do token se parece com um número, URL, e-mail </td> </tr>\n",
    "<tr> <td> <span> `POS`,` TAG`, `DEP`,` LEMMA`, `SHAPE` </span> </td> <td> A tag de classe gramatical simples e estendida do token , rótulo de dependência, lema, forma </td> </tr>\n",
    "<tr> <td> <span> `ENT_TYPE` </span> </td> <td> O rótulo da entidade do token </td> </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token curinga\n",
    "Você pode passar um dicionário vazio `{}` como um curinga para representar ** qualquer token **. Por exemplo, você pode querer recuperar hashtags sem saber o que pode seguir o caractere `#`:\n",
    "> `[{'ORTH': '#'}, {}]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PhraseMatcher\n",
    "Na seção acima, usamos padrões de token para realizar a correspondência baseada em regras. Um método alternativo - e geralmente mais eficiente - é fazer a correspondência em listas de terminologia. Neste caso, usamos PhraseMatcher para criar um objeto Doc a partir de uma lista de frases e passá-lo para o `matcher`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('/home/alana/Documentos/NLP/UDEMY/en_core_web_sm-2.2.0/en_core_web_sm/en_core_web_sm-2.2.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_clean_texto(path):\n",
    "    \n",
    "    with open(path, 'r') as f:\n",
    "        texto = f.read()\n",
    "        texto = texto.lower()\n",
    "        texto = re.sub(r'\\n+|\\t+', ' ' ,texto)\n",
    "        texto = texto.encode('ascii', 'ignore')\n",
    "        texto = texto.decode(\"utf-8\")\n",
    "        texto = unicodedata.normalize('NFD', texto)\n",
    "        \n",
    "        return str(texto) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'reaganomics.txt'\n",
    "doc3 = nlp(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primeiro, crie uma lista de frases correspondentes:\n",
    "phrase_list = ['voodoo economics', 'supply-side economics', 'trickle-down economics', 'free-market economics']\n",
    "\n",
    "# Em seguida, converta cada frase em um objeto Doc:\n",
    "phrase_patterns = [nlp(text) for text in phrase_list]\n",
    "\n",
    "# Passe cada objeto Doc para o matcher (observe o uso do asterisco!):\n",
    "matcher.add('VoodooEconomics', None, *phrase_patterns)\n",
    "\n",
    "# Construa uma lista de correspondências:\n",
    "matches = matcher(doc3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3473369816841043438, 40, 44),\n",
       " (3473369816841043438, 48, 52),\n",
       " (3473369816841043438, 53, 55),\n",
       " (3473369816841043438, 60, 64),\n",
       " (3473369816841043438, 664, 668),\n",
       " (3473369816841043438, 2960, 2964)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# match_id, inicio, fim\n",
    "matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    As primeiras quatro correspondências são onde esses termos são usados na definição de Reaganomics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reaganomics https://en.wikipedia.org/wiki/reaganomics reaganomics (a portmanteau of [ronald] reagan and economics attributed to paul harvey)[1] refers to the economic policies promoted by u.s. president ronald reagan during the 1980s. these policies are commonly associated with supply-side economics, referred to as trickle-down economics or voodoo economics by political opponents, and free-market economics by political advocates. the four"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3[:70]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Viewing Partidas\n",
    "    Existem algumas maneiras de buscar o texto em torno de uma correspondência. O mais simples é pegar uma fatia de tokens do documento que seja mais larga do que a correspondência:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "supply-side economics movement, which formed in opposition to keynesian demand-stimulus economics. this movement produced some"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3[665:685]  # Observe que a quinta partida começa no doc3 [664]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "brackets, as that extra money for the wealthy could trickle along to low-income groups.[67] federal income"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3[2975:2995]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Outra maneira é primeiro aplicar o sentencizer ao Doc e, em seguida, iterar através das frases até o match point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 34\n"
     ]
    }
   ],
   "source": [
    "# Construa uma lista de frases\n",
    "sents = [sent for sent in doc3.sents]\n",
    "\n",
    "# Na próxima seção, veremos que as frases contêm valores de token inicial e final:\n",
    "print(sents[0].start, sents[0].end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at the same time he attracted a following from the supply-side economics movement, which formed in opposition to keynesian demand-stimulus economics.\n"
     ]
    }
   ],
   "source": [
    "# Repita a lista de frases até que o valor final da frase exceda um valor inicial de correspondência:\n",
    "for sent in sents:\n",
    "    if matches[4][1] < sent.end:  # esta é a quinta partida, que começa no doc3 [664]\n",
    "        print(sent)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spacy.io/usage/linguistic-features#section-rule-based-matching"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
